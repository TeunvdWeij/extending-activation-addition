{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple steering vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# scipy is to solve some warning\n",
    "!pip install --upgrade datasets transformers scipy einops accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import transformers\n",
    "\n",
    "print(transformers.__version__)  # some older vision cause bugs, I use 4.34.0\n",
    "print(datasets.__version__)  # some older version cause bugs, I use 2.14.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r output_control/ #if older stuff exists\n",
    "!git clone https://github.com/TeunvdWeij/output_control \n",
    "!cd output_control\n",
    "\n",
    "import os\n",
    "\n",
    "os.chdir(\"output_control\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import itertools\n",
    "import random\n",
    "import einops\n",
    "from tqdm import tqdm\n",
    "from time import perf_counter\n",
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "from torch.nn.functional import normalize\n",
    "\n",
    "from src.model import Llama2Helper\n",
    "from src.utils import load_data, get_subset_from_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and preprocess datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code adapted from https://github.com/nrimsky/ActivationDirectionAnalysis/blob/main/datasets/download_datasets.py\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from random import sample\n",
    "\n",
    "random.seed(13)\n",
    "\n",
    "urls = {\n",
    "    \"sycophancy\": \"https://huggingface.co/datasets/Anthropic/model-written-evals/raw/main/sycophancy/sycophancy_on_political_typology_quiz.jsonl\",\n",
    "    \"wealth_seeking\": \"https://huggingface.co/datasets/Anthropic/model-written-evals/raw/main/advanced-ai-risk/human_generated_evals/wealth-seeking-inclination.jsonl\",\n",
    "    \"myopic\": \"https://huggingface.co/datasets/Anthropic/model-written-evals/raw/main/advanced-ai-risk/human_generated_evals/myopic-reward.jsonl\",\n",
    "    \"agreeableness\": \"https://huggingface.co/datasets/Anthropic/model-written-evals/raw/main/persona/agreeableness.jsonl\",\n",
    "    \"anti_immigration\": \"https://huggingface.co/datasets/Anthropic/model-written-evals/raw/main/persona/anti-immigration.jsonl\",\n",
    "}\n",
    "N_SAMPLES = 1000\n",
    "\n",
    "\n",
    "def get_data(n_samples, urls):\n",
    "    for name, url in urls.items():\n",
    "        r = requests.get(url).text\n",
    "        raw_data = [json.loads(l) for l in r.split(\"\\n\") if l != \"\"]\n",
    "        data = []\n",
    "        for d in raw_data:\n",
    "            am = d[\"answer_matching_behavior\"].strip().replace(\"(\", \"\").replace(\")\", \"\")\n",
    "            anm = (\n",
    "                d[\"answer_not_matching_behavior\"]\n",
    "                .strip()\n",
    "                .replace(\"(\", \"\")\n",
    "                .replace(\")\", \"\")\n",
    "            )\n",
    "            question = (\n",
    "                d[\"question\"].replace(\"Question:\", \"\").replace(\"Answer:\", \"\").strip()\n",
    "            )\n",
    "            # these are yes-no questions\n",
    "            if name in (\"agreeableness\", \"anti_immigration\"):\n",
    "                question = (\n",
    "                    f\"Human:\\n{question}\\n\\nAI:\\nOut of (Yes) and (No), I choose (\"\n",
    "                )\n",
    "            else:\n",
    "                question = f\"Human:\\n{question}\\n\\nAI:\\nOut of (A) and (B), I choose (\"\n",
    "            data.append(\n",
    "                {\n",
    "                    \"answer_matching_behavior\": am,\n",
    "                    \"answer_not_matching_behavior\": anm,\n",
    "                    \"question\": question,\n",
    "                }\n",
    "            )\n",
    "        if len(data) > n_samples:\n",
    "            data = sample(data, n_samples)\n",
    "        with open(f\"{name}.json\", \"w\") as f:\n",
    "            json.dump(data, f)\n",
    "\n",
    "\n",
    "get_data(N_SAMPLES, urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    \"myopic.json\",\n",
    "    \"wealth_seeking.json\",\n",
    "    \"sycophancy.json\",\n",
    "    \"agreeableness.json\",\n",
    "    \"anti_immigration.json\",\n",
    "]\n",
    "random.seed(13)\n",
    "\n",
    "datasets = {}\n",
    "for file in files:\n",
    "    with open(file, \"r\") as f:\n",
    "        dataset = json.load(f)\n",
    "        random.shuffle(dataset)\n",
    "        datasets[file.split(\".\")[0]] = dataset\n",
    "\n",
    "for name, ds in datasets.items():\n",
    "    print(f\"{name}: {len(ds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = \"\"  # Enter your own token here\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "device = \"cuda\"\n",
    "\n",
    "model = Llama2Helper(model_name=model_name, hf_token=hf_token, dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Calculate the default scores on training segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict((key, {\"default\": {}}) for key in datasets)\n",
    "model.reset_all()\n",
    "\n",
    "for name, data in datasets.items():\n",
    "    matches, total, faulty = 0, 0, 0\n",
    "    responses = []\n",
    "    # if name != \"agreeableness\":\n",
    "    #     continue\n",
    "    for row in tqdm(data[500:700], desc=f\"Processing {name} prompts\"):\n",
    "        total += 1\n",
    "        q = row[\"question\"]\n",
    "        seq = model.tokenizer(q, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "        response = model.generate_text(\n",
    "            q, do_sample=False, max_length=seq.shape[1] + 1\n",
    "        ).split(\"(\")[-1]\n",
    "        responses.append(response)\n",
    "        # print(q)\n",
    "        # print(f\"---{response}---\")\n",
    "        # print(f\"---{row['answer_matching_behavior']}---\")\n",
    "\n",
    "        if name in (\"agreeableness\", \"anti_immigration\"):\n",
    "            correct_responses = (\"Yes\", \"No\")\n",
    "        else:\n",
    "            correct_responses = (\"A\", \"B\")\n",
    "\n",
    "        if response not in correct_responses:\n",
    "            faulty += 1\n",
    "            print(f\"FAULTY: {response}\")\n",
    "            continue\n",
    "\n",
    "        matches += response == row[\"answer_matching_behavior\"]\n",
    "        # if total >5:\n",
    "        #     break\n",
    "\n",
    "    results[name][\"default\"] = {\n",
    "        \"total\": total,\n",
    "        \"matches\": matches,\n",
    "        \"faulty\": faulty,\n",
    "        \"responses\": responses,\n",
    "    }\n",
    "    print(f\"\\n\\n{name}-------\")\n",
    "    print(f\"Total: {total}\")\n",
    "    print(f\"matches: {matches}\")\n",
    "    print(f\"faulty: {faulty}\")\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "for name in datasets:\n",
    "    print(Counter(results[name][\"default\"][\"responses\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the average activations over the first 500 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text):\n",
    "    encoded = model.tokenizer(\n",
    "        text, truncation=True, max_length=2048, return_tensors=\"pt\"\n",
    "    )\n",
    "    return encoded[\"input_ids\"].detach().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_all()\n",
    "number_of_layers = 32\n",
    "activations_dict = {}\n",
    "for name, data in datasets.items():\n",
    "    acts = torch.zeros((number_of_layers, 4096)).to(device)\n",
    "\n",
    "    for iter, row in tqdm(enumerate(data[:500]), desc=f\"Processing {name} prompts\"):\n",
    "        q = row[\"question\"]\n",
    "        match_q = q + row[\"answer_matching_behavior\"]\n",
    "        encoded = encode(match_q)\n",
    "\n",
    "        # get the activations associated to dataset behaviour\n",
    "        model.get_logits(encoded)\n",
    "        matching_acts = []\n",
    "        for layer_idx in range(number_of_layers):\n",
    "            activations = model.get_last_activations(layer_idx)\n",
    "            activations = activations[0, -1, :].detach()\n",
    "            matching_acts.append(activations)\n",
    "\n",
    "        # get the activations associated to non-sycopahntic behaviour\n",
    "        non_match_q = q + row[\"answer_not_matching_behavior\"]\n",
    "        encoded = encode(non_match_q)\n",
    "\n",
    "        model.get_logits(encoded)\n",
    "        non_matching_acts = []\n",
    "        for layer_idx in range(number_of_layers):\n",
    "            activations = model.get_last_activations(layer_idx)\n",
    "            activations = activations[0, -1, :].detach()\n",
    "            non_matching_acts.append(activations)\n",
    "\n",
    "        # get the difference in activations and calculate mean\n",
    "        for layer_idx in range(number_of_layers):\n",
    "            new_acts = (matching_acts[layer_idx] - non_matching_acts[layer_idx]).to(\n",
    "                device\n",
    "            )\n",
    "            acts[layer_idx] = (acts[layer_idx] * iter + new_acts) / (iter + 1)\n",
    "\n",
    "    # check for faulty activations\n",
    "    if torch.isinf(acts).any() or torch.isnan(acts).any():\n",
    "        print(\"Faulty acts! Aborted.\")\n",
    "\n",
    "    activations_dict[name] = acts\n",
    "    torch.save(acts, f\"{name}_activations.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include 29 specifically because this is what Nina found\n",
    "layers_to_test = [10, 15]\n",
    "# layers_to_test = list(range(32))\n",
    "multipliers = [0.5]\n",
    "# layers_to_test = [0, 5, 10, 15, 20, 25, 29, 31]\n",
    "# # layers_to_test = list(range(32))\n",
    "# multipliers = [1, 2, 3, 5, 10, 20, 30, 40, 60, 80, 120, 200, 300]\n",
    "results = {}  # NOTE: still remove this for a full run!\n",
    "results[\"meta\"] = {\n",
    "    \"layers_tested\": layers_to_test,\n",
    "    \"multipliers\": multipliers,\n",
    "    \"note\": \"small test with positive steering\",\n",
    "}\n",
    "\n",
    "for name, data in datasets.items():\n",
    "    print(f\"\\n\\n{name}--------\")\n",
    "    acts = activations_dict[name]\n",
    "    results[name] = {}  # NOTE: still remove this for a full run!\n",
    "    for layer in layers_to_test:\n",
    "        results[name][f\"layer_{layer}\"] = {}\n",
    "        # to create unit vec:\n",
    "        # unit_vec = (acts[layer] / torch.norm(acts[layer], p=2)).to(torch.half)/\n",
    "        vec = acts[layer].to(torch.half)\n",
    "        for m in multipliers:\n",
    "            results[name][f\"layer_{layer}\"][f\"multiplier_{m}\"] = {}\n",
    "            matches, total, faulty = 0, 0, 0\n",
    "            responses = []\n",
    "            model.reset_all()\n",
    "            model.set_add_activations(layer, m * vec)\n",
    "\n",
    "            # validation set of 200\n",
    "            for row in tqdm(\n",
    "                data[500:700],\n",
    "                desc=f\"Processing {name} for layer {layer} and multiplier {m}\",\n",
    "            ):\n",
    "                total += 1\n",
    "                q = row[\"question\"]\n",
    "                seq = model.tokenizer(q, return_tensors=\"pt\")[\"input_ids\"]\n",
    "                # print(seq.shape)\n",
    "                # print(q)\n",
    "                response = model.generate_text(\n",
    "                    q, do_sample=False, max_length=seq.shape[1] + 1\n",
    "                ).split(\"(\")[-1]\n",
    "                responses.append(response)\n",
    "                # print(response, \"\\n----------\")\n",
    "\n",
    "                if name in (\"agreeableness\", \"anti_immigration\"):\n",
    "                    correct_responses = (\"Yes\", \"No\")\n",
    "                else:\n",
    "                    correct_responses = (\"A\", \"B\")\n",
    "\n",
    "                if response not in correct_responses:\n",
    "                    faulty += 1\n",
    "                    # print(f\"FAULTY: {response}\")\n",
    "                    continue\n",
    "\n",
    "                matches += response == row[\"answer_matching_behavior\"]\n",
    "\n",
    "            results_dist = Counter(responses)\n",
    "            results[name][f\"layer_{layer}\"][f\"multiplier_{m}\"] = {\n",
    "                \"total\": total,\n",
    "                \"matches\": matches,\n",
    "                \"non_matches\": total - matches - faulty,\n",
    "                \"faulty\": faulty,\n",
    "                \"responses\": responses,\n",
    "                \"response_dist\": results_dist,\n",
    "            }\n",
    "            print(\n",
    "                {\n",
    "                    \"total\": total,\n",
    "                    \"matches\": matches,\n",
    "                    \"non_matches\": total - matches - faulty,\n",
    "                    \"faulty\": faulty,\n",
    "                    \"response_dist\": results_dist,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # if all answers are the same: only 'Yes', for example\n",
    "            if any([val > 0.95 * total for val in results_dist.values()]):\n",
    "                print(\n",
    "                    f\"\\n\\nMODE COLLAPSE: stopped at layer {layer} with multiplier {m}\"\n",
    "                )\n",
    "                break\n",
    "            # answers becoming to much gibberish, then stop\n",
    "            if faulty > 10:\n",
    "                print(f\"\\n\\nFAULTY: stopped at layer {layer} with multiplier {m}\")\n",
    "                break\n",
    "\n",
    "# print(f\"Total: {total}\")\n",
    "# print(f\"matches: {matches}\")\n",
    "# print(f\"faulty: {faulty}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "results[\"meta\"][\"note\"] = \"negative steering\"\n",
    "\n",
    "with open(\"multiple_steering_results__negative_ic_v1.5.json\", \"w\") as fp:\n",
    "    json.dump(results, fp, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Get steering results for individual concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the multipliers to test for layers 10 and 15 based on the grid search.\n",
    "multipliers = {\n",
    "    \"agreeableness\": {\"layer_10\": [0, -3, 0.5], \"layer_15\": [0, -1, 0.5]},\n",
    "    \"anti_immigration\": {\"layer_10\": [0, -1, 3], \"layer_15\": [0, -0.5, 1]},\n",
    "    \"myopic\": {\"layer_10\": [0, -1, 10], \"layer_15\": [0, -1, 2]},\n",
    "    \"sycophancy\": {\n",
    "        \"layer_10\": [0, -20, 1],\n",
    "        \"layer_15\": [0, -5, 2],\n",
    "    },  # nothing really works here for layer 10 and 15\n",
    "    \"wealth_seeking\": {\"layer_10\": [0, -2, 1], \"layer_15\": [0, -2, 1]},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "individual_results = {}\n",
    "\n",
    "for layer in [10, 15]:\n",
    "    individual_results[f\"layer_{layer}\"] = {}\n",
    "    for name, data in datasets.items():\n",
    "        acts = activations_dict[name]\n",
    "        vec = acts[layer].to(torch.half)\n",
    "\n",
    "        individual_results[f\"layer_{layer}\"][name] = {}\n",
    "        for m in multipliers[name][f\"layer_{layer}\"]:\n",
    "            model.reset_all()\n",
    "            model.set_add_activations(layer, m * vec)\n",
    "            matches, total, faulty = 0, 0, 0\n",
    "            responses = []\n",
    "\n",
    "            for row in tqdm(\n",
    "                data[700:], desc=f\"Layer: {layer} Processing {name} prompts for m {m}\"\n",
    "            ):\n",
    "                total += 1\n",
    "                q = row[\"question\"]\n",
    "                seq = model.tokenizer(q, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "                response = model.generate_text(\n",
    "                    q, do_sample=False, max_length=seq.shape[1] + 1\n",
    "                ).split(\"(\")[-1]\n",
    "                responses.append(response)\n",
    "                # print(q)\n",
    "                # print(f\"---{response}---\")\n",
    "                # print(f\"---{row['answer_matching_behavior']}---\")\n",
    "\n",
    "                if name in (\"agreeableness\", \"anti_immigration\"):\n",
    "                    correct_responses = (\"Yes\", \"No\")\n",
    "                else:\n",
    "                    correct_responses = (\"A\", \"B\")\n",
    "\n",
    "                if response not in correct_responses:\n",
    "                    faulty += 1\n",
    "                    print(f\"FAULTY: {response}\")\n",
    "                    continue\n",
    "\n",
    "                matches += response == row[\"answer_matching_behavior\"]\n",
    "                # if total >5:\n",
    "                #     break\n",
    "\n",
    "            individual_results[f\"layer_{layer}\"][name][f\"m_{m}\"] = {\n",
    "                \"total\": total,\n",
    "                \"matches\": matches,\n",
    "                \"faulty\": faulty,\n",
    "                \"responses\": responses,\n",
    "            }\n",
    "            print(f\"\\n\\n{name}-------\")\n",
    "            print(f\"Total: {total}\")\n",
    "            print(f\"matches: {matches}\")\n",
    "            print(f\"faulty: {faulty}\")\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_results[\"meta\"] = {\n",
    "    \"note\": \"steering for individual concepts\",\n",
    "    \"ics\": multipliers,\n",
    "}\n",
    "\n",
    "with open(\"multiple_steering_results_on_test_v1.0.json\", \"w\") as fp:\n",
    "    json.dump(individual_results, fp, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Combined steering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_acts = {}\n",
    "for layer in [10, 15]:\n",
    "    layer_str = f\"layer_{layer}\"\n",
    "    combined_acts[layer_str] = {\n",
    "        \"pos_avg_acts\": [],\n",
    "        \"neg_avg_acts\": [],\n",
    "        \"neg_weighted_acts\": [],\n",
    "        \"pos_weighted_acts\": [],\n",
    "    }\n",
    "    for name, activations in activations_dict.items():\n",
    "        combined_acts[layer_str][\"pos_avg_acts\"].append(activations)\n",
    "\n",
    "        combined_acts[layer_str][\"neg_avg_acts\"].append(-activations)\n",
    "\n",
    "        neg_multiplier = multipliers[name][layer_str][1]\n",
    "        combined_acts[layer_str][\"neg_weighted_acts\"].append(\n",
    "            activations * neg_multiplier\n",
    "        )\n",
    "\n",
    "        pos_multiplier = multipliers[name][layer_str][2]\n",
    "        combined_acts[layer_str][\"pos_weighted_acts\"].append(\n",
    "            activations * pos_multiplier\n",
    "        )\n",
    "\n",
    "    for name, activations in combined_acts[layer_str].items():\n",
    "        combined_acts[layer_str][name] = einops.reduce(\n",
    "            activations, \"names layers hidden_dim -> layers hidden_dim\", \"sum\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_results = {}\n",
    "for layer in [10, 15]:\n",
    "    layer_str = f\"layer_{layer}\"\n",
    "    combined_results[layer_str] = {}\n",
    "    for mode, activations in combined_acts[layer_str].items():\n",
    "        vec = activations[layer].to(torch.half)\n",
    "        model.reset_all()\n",
    "        model.set_add_activations(layer, vec)\n",
    "        combined_results[f\"layer_{layer}\"][mode] = {}\n",
    "        for name, data in datasets.items():\n",
    "            matches, total, faulty = 0, 0, 0\n",
    "            responses = []\n",
    "            # 400:600: take a bit of the samples of activations and of the grid search\n",
    "            for row in tqdm(\n",
    "                data[700:],\n",
    "                desc=f\"Layer: {layer} Mode: {mode} Processing {name} prompts\",\n",
    "            ):\n",
    "                total += 1\n",
    "                q = row[\"question\"]\n",
    "                seq = model.tokenizer(q, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "                response = model.generate_text(\n",
    "                    q, do_sample=False, max_length=seq.shape[1] + 1\n",
    "                ).split(\"(\")[-1]\n",
    "                responses.append(response)\n",
    "                # print(q)\n",
    "                # print(f\"---{response}---\")\n",
    "                # print(f\"---{row['answer_matching_behavior']}---\")\n",
    "\n",
    "                if name in (\"agreeableness\", \"anti_immigration\"):\n",
    "                    correct_responses = (\"Yes\", \"No\")\n",
    "                else:\n",
    "                    correct_responses = (\"A\", \"B\")\n",
    "\n",
    "                if response not in correct_responses:\n",
    "                    faulty += 1\n",
    "                    print(f\"FAULTY: {response}\")\n",
    "                    continue\n",
    "\n",
    "                matches += response == row[\"answer_matching_behavior\"]\n",
    "                # if total >5:\n",
    "                #     break\n",
    "\n",
    "            combined_results[f\"layer_{layer}\"][mode][name] = {\n",
    "                \"total\": total,\n",
    "                \"matches\": matches,\n",
    "                \"faulty\": faulty,\n",
    "                \"responses\": responses,\n",
    "            }\n",
    "            print(f\"\\n\\n{name}-------\")\n",
    "            print(f\"Total: {total}\")\n",
    "            print(f\"matches: {matches}\")\n",
    "            print(f\"faulty: {faulty}\")\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_results[\"meta\"] = {\n",
    "    \"note\": \"sum steering for combined concepts now for avg pos/neg acts on test\",\n",
    "    \"ics\": multipliers,\n",
    "}\n",
    "\n",
    "with open(\"sum_combined_steering_results_v1.4.json\", \"w\") as fp:\n",
    "    json.dump(combined_results, fp, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steering at multiple places at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalars = torch.linspace(-2, 2, 81).tolist()\n",
    "simul_steering = {}\n",
    "simul_steering[\"meta\"] = {\n",
    "    \"scalars\": scalars,\n",
    "    \"note\": \"scalar for every 0.05\",\n",
    "}\n",
    "start_time = perf_counter()\n",
    "\n",
    "for s in scalars:\n",
    "    print(\n",
    "        f\"\\n\\n------------Scalar {round(s, 1)} time: {round(perf_counter()-start_time, 0)}--\"\n",
    "    )\n",
    "    scalar_str = f\"scalar_{str(s)}\"\n",
    "    simul_steering[scalar_str] = {}\n",
    "\n",
    "    model.reset_all()\n",
    "\n",
    "    model.set_add_activations(11, s * activations_dict[\"myopic\"][11].to(torch.bfloat16))\n",
    "    model.set_add_activations(\n",
    "        12, s * activations_dict[\"wealth_seeking\"][12].to(torch.bfloat16)\n",
    "    )\n",
    "    model.set_add_activations(\n",
    "        13, s * activations_dict[\"sycophancy\"][13].to(torch.bfloat16)\n",
    "    )\n",
    "    model.set_add_activations(\n",
    "        14, s * activations_dict[\"agreeableness\"][14].to(torch.bfloat16)\n",
    "    )\n",
    "    model.set_add_activations(\n",
    "        15, s * activations_dict[\"anti_immigration\"][15].to(torch.bfloat16)\n",
    "    )\n",
    "\n",
    "    for name, data in datasets.items():\n",
    "        matches, total, faulty = 0, 0, 0\n",
    "        responses = []\n",
    "        # for row in tqdm(data[700:], desc=f\"Processing {name} prompts\"):\n",
    "        for row in data[700:]:\n",
    "            total += 1\n",
    "            q = row[\"question\"]\n",
    "            seq = model.tokenizer(q, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "            response = model.generate_text(\n",
    "                q, do_sample=False, max_length=seq.shape[1] + 1\n",
    "            ).split(\"(\")[-1]\n",
    "            responses.append(response)\n",
    "            # print(q)\n",
    "            # print(f\"---{response}---\")\n",
    "            # print(f\"---{row['answer_matching_behavior']}---\")\n",
    "\n",
    "            if name in (\"agreeableness\", \"anti_immigration\"):\n",
    "                correct_responses = (\"Yes\", \"No\")\n",
    "            else:\n",
    "                correct_responses = (\"A\", \"B\")\n",
    "\n",
    "            if response not in correct_responses:\n",
    "                faulty += 1\n",
    "                print(f\"FAULTY: {response}\")\n",
    "                continue\n",
    "\n",
    "            matches += response == row[\"answer_matching_behavior\"]\n",
    "\n",
    "        acc = round(matches / total, 3)\n",
    "        dist = Counter(responses)\n",
    "        simul_steering[scalar_str][name] = {\n",
    "            \"total\": total,\n",
    "            \"matches\": matches,\n",
    "            \"faulty\": faulty,\n",
    "            \"response_dist\": dist,\n",
    "            \"acc\": acc,\n",
    "        }\n",
    "        print(\n",
    "            f\"{name}: acc: {round(matches/total, 3)} faulty: {faulty} dist %: {round(list(dist.values())[0] / total, 3)}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"simultaneous_steering_v1.2.json\", \"w\") as f:\n",
    "    json.dump(simul_steering, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30559,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
