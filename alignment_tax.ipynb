{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/nrimsky/LM-exp/blob/main/steering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{},"source":["# Steering experiments\n","Code based on https://github.com/nrimsky/LM-exp/blob/main/sycophancy/sycophancy_steering.ipynb. \n"]},{"cell_type":"code","execution_count":18,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","from tqdm import tqdm\n","\n","# hf_token = input(\"Enter your huggingface token to use the Llama2 models: \") "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4NKVV3TfeQUi","trusted":true},"outputs":[],"source":["with open(\"private_information/hf_token\", \"r\") as f:\n","    hf_token = f.read()\n","\n","class BlockOutputWrapper(torch.nn.Module):\n","    def __init__(self, block):\n","        super().__init__()\n","        self.block = block\n","        self.last_hidden_state = None\n","        self.add_activations = None\n","        self.output_init = None\n","\n","    def forward(self, *args, **kwargs):\n","        output = self.block(*args, **kwargs)\n","        self.last_hidden_state = output[0]\n","        self.output_before_adding = output\n","        if self.add_activations is not None:\n","            output = (output[0] + self.add_activations,) + output[1:]\n","        self.output_after_adding = output\n","        return output\n","\n","    def add(self, activations):\n","        self.add_activations = activations\n","\n","    def reset(self):\n","        self.last_hidden_state = None\n","        self.add_activations = None\n","\n","    \n","class Llama27BHelper:\n","    def __init__(self, pretrained_model=\"meta-llama/Llama-2-7b-hf\"):\n","        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model, device_map=\"auto\", use_auth_token=hf_token, torch_dtype=torch.half)\n","        self.model = AutoModelForCausalLM.from_pretrained(pretrained_model, device_map=\"auto\", use_auth_token=hf_token, torch_dtype=torch.half)#.to(self.device)\n","        for i, layer in enumerate(self.model.model.layers):\n","            self.model.model.layers[i] = BlockOutputWrapper(layer)\n","\n","    def generate_text(self, prompt, do_sample=False, temperature=1., max_length=100):\n","        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n","        generate_ids = self.model.generate(inputs.input_ids.to(self.device), do_sample=do_sample, temperature=temperature,max_length=max_length)\n","        return self.tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n","    \n","    def get_logits(self, tokens):\n","        with torch.no_grad():\n","            return self.model(tokens.to(self.device)).logits \n","    \n","    def get_last_activations(self, layer):\n","        return self.model.model.layers[layer].last_hidden_state\n","\n","    def set_add_activations(self, layer, activations):\n","        self.model.model.layers[layer].add(activations)\n","\n","    def reset_all(self):\n","        for layer in self.model.model.layers:\n","            layer.reset()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model_name = \"meta-llama/Llama-2-7b-hf\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=hf_token)\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","model = Llama27BHelper(model_name)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'AutoTokenizer' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m/home/mailv/documents/output_control/alignment_tax.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/mailv/documents/output_control/alignment_tax.ipynb#Y132sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/mailv/documents/output_control/alignment_tax.ipynb#Y132sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     model_name,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/mailv/documents/output_control/alignment_tax.ipynb#Y132sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     device_map\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/mailv/documents/output_control/alignment_tax.ipynb#Y132sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     use_auth_token\u001b[39m=\u001b[39mhf_token,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/mailv/documents/output_control/alignment_tax.ipynb#Y132sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     torch_dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mhalf,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/mailv/documents/output_control/alignment_tax.ipynb#Y132sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m )\n","\u001b[0;31mNameError\u001b[0m: name 'AutoTokenizer' is not defined"]}],"source":["tokenizer = AutoTokenizer.from_pretrained(\n","    model_name,\n","    device_map=\"auto\",\n","    use_auth_token=hf_token,\n","    torch_dtype=torch.half,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluate performance on the Pile"]},{"cell_type":"code","execution_count":1,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/mailv/documents/output_control/output_control_venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","Resolving data files: 100%|██████████| 30/30 [00:00<00:00, 67.67it/s]\n"]}],"source":["# adapted from https://github.com/pesvut/separability/blob/b435310c5728dcfacb0312799d98ba6e7146507c/src/separability/texts.py#L3  \n","from datasets import load_dataset\n","\n","def load_pile(split, codeless=False):\n","    dataset = load_dataset(\"monology/pile-uncopyrighted\", streaming=True, split=split)\n","    \n","    if codeless:\n","        def filter_out_code(example):\n","            return example['meta']['pile_set_name'] != 'Github'\n","    \n","        dataset = dataset.filter(filter_out_code)\n","    \n","    return dataset\n","\n","dataset = load_pile(\"validation\", codeless=False)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'dict'>\n","\n","dict_keys(['text', 'meta'])\n","\n","{'text': 'Effect of sleep quality on memory, executive function, and language performance in patients with refractory focal epilepsy and controlled epilepsy versus healthy controls - A prospective study.\\nWe aimed to evaluate the effect of sleep quality on memory, executive function, and language performance in patients with refractory focal epilepsy and controlled epilepsy and compare these with healthy individuals. We prospectively enrolled 37 adolescent and adult patients with refractory focal epilepsy (Group 1) and controlled epilepsy (Group 2) in each group. History pertaining to epilepsy and sleep were recorded, and all patients underwent overnight polysomnography. Language, memory, and executive function assessments were done using Western Aphasia Battery, Post Graduate Institute (PGI) memory scale, and battery of four executive function tests (Trail Making Test A & B, Digit symbol test, Stroop Task, and Verbal Fluency Test), respectively. Forty age- and sex-matched controls were also included in the study. Significant differences were noted in both objective and subjective sleep parameters among all the groups. On polysomnography, parameters like total sleep time, sleep efficiency, sleep latency, and rapid eye movement (REM) latency were found to be significantly worse in Group 1 as compared with Group 2. Cognitive and executive parameters were significantly impaired in Group 1. Shorter total sleep time, poorer sleep efficiency, and prolonged sleep latencies were observed to be associated with poor memory and executive function in patients with refractory epilepsy. Our study strongly suggests that sleep disturbances, mainly shorter total sleep time, poor sleep efficiency, and prolonged sleep latencies, are associated with impaired memory and executive function in patients with refractory focal epilepsy and to a lesser extent, among those with medically controlled epilepsy.', 'meta': {'pile_set_name': 'PubMed Abstracts'}}\n"]}],"source":["for batch in dataset:\n","    # print(type(batch), batch.keys(), batch, sep=\"\\n\\n\")\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["text = \"Your text here\"\n","inputs = tokenizer.encode(text, return_tensors=\"pt\", max_length=512, truncation=True).cuda()\n","\n","logits = model.get_logits(inputs)\n","\n","# Compute the model predicted tokens\n","predicted_indices = logits.argmax(dim=-1)\n","\n","# Decode predicted tokens\n","predicted_text = tokenizer.decode(predicted_indices[0])\n","\n","print(predicted_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(inputs)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["logits.shape"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import torch"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[[8, 6, 4, 9, 4, 1, 2, 1],\n","         [9, 5, 5, 2, 3, 3, 6, 5],\n","         [7, 7, 4, 1, 6, 3, 9, 8],\n","         [1, 6, 4, 5, 7, 5, 3, 4]],\n","\n","        [[2, 2, 7, 7, 2, 1, 5, 7],\n","         [9, 3, 7, 6, 1, 4, 3, 4],\n","         [1, 9, 3, 9, 4, 9, 6, 5],\n","         [8, 6, 2, 8, 7, 2, 8, 5]]])"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["a = torch.randint(1, 10, (2, 4, 8))\n","a"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([0, 7, 7, 7, 8, 8, 7, 8, 5, 7])"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["b = a.flatten()\n","b.bincount()"]},{"cell_type":"markdown","metadata":{},"source":["## Activation Arithmetic\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["d = {}\n","cos = torch.nn.CosineSimilarity(dim=-1, eps=1e-6)\n","\n","if model_name == \"meta-llama/Llama-2-7b-hf\":\n","    layer = 29\n","    acts_size = 4096\n","elif model_name == \"meta-llama/Llama-2-13b-hf\":\n","    layer = 35\n","    acts_size = 5120 # i think this is wrong \n","else: \n","    print(\"specify dimensions\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["base = model.model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["base.generate(tokenizer.encode(\"\", return_tensors=\"pt\").to(\"cuda\"), do_sample=False, temperature=0., max_length=5)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["tokenizer.encode(\"\", return_tensors=\"pt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# model.reset_all()\n","lissie = [] \n","for _ in range(5):\n","    model.generate_text(\"\", do_sample=True, max_length=0, temperature=0.5)\n","    lissie.append(model.get_last_activations(28)[0, -1, :].detach())\n","    \n","for i in range(4):\n","    print(torch.allclose(lissie[i], lissie[i+1]))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# model.reset_all()\n","lissie = [] \n","for _ in range(5):\n","    print(model.generate_text(\"\", do_sample=True, max_length=3, temperature=0.5))\n","    lissie.append(model.get_last_activations(28)[0, -1, :].detach())\n","    \n","for i in range(4):\n","    print(torch.allclose(lissie[i], lissie[i+1]))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["inputs = {\"horn\", \"crocodile\", \"tears\", \"love\", \"hate\"}\n","\n","for seq in inputs:\n","    if seq in d:\n","        continue\n","        \n","    model.reset_all()\n","    model.get_logits(tokenizer.encode(seq, return_tensors=\"pt\"))\n","    # get the activations of the last token because that carries most of the relevant context\n","    d[seq] = model.get_last_activations(layer)[0, -1, :].detach()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["d['crocodile'].shape"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["cos((d[\"crocodile\"] - d[\"horn\"]), d[\"tears\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["cos(d[\"crocodile\"], d[\"tears\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["tokenizer.tokenize(\"elephant\"), tokenizer.tokenize(\"crocodile\"), tokenizer.tokenize(\"rhinoceros\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["neg_inputs = [\"horn\", \"the horn\", \"a horn\"]\n","neg_acts = torch.zeros((1, 1, acts_size))\n","for seq in neg_inputs:\n","    model.reset_all()\n","    model.get_logits(tokenizer.encode(seq, return_tensors=\"pt\"))\n","    # get the activations of the last token because that carries most of the relevant context\n","    neg_acts += model.get_last_activations(layer)[0, -1, :].detach().cpu()\n","    \n","neg_acts = neg_acts / len(neg_inputs)\n","neg_acts = neg_acts.to(torch.half)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.reset_all()\n","\n","# inputs = [\"Cow\", \"the cow\", \"cow\", \"A cow\", \"a cow\"]\n","# inputs = [\"Elephant\", \"the elephant\", \"elephant\", \"an elephant\"]\n","inputs = [\"crocodile\", \"the crocodile\", \"a crocodile\", \"a crocodile\"]\n","# inputs = [\"rhinoceros\", \"the rhinoceros\", \"a rhinoceros\", \"a rhinoceros\"]\n","\n","# inputs = [\"cow\"]\n","multipliers = [0, 0.5, 1, 5, 10, 15]\n","\n","acts = torch.zeros((1, 1, acts_size))\n","for seq in inputs:\n","    model.reset_all()\n","    model.get_logits(tokenizer.encode(seq, return_tensors=\"pt\"))\n","    # get the activations of the last token because that carries most of the relevant context\n","    acts += model.get_last_activations(layer)[0, -1, :].detach().cpu()\n","\n","acts = acts / len(inputs)\n","acts = acts.to(torch.half)\n","\n","for m in multipliers:\n","    print(f\"\\n-----{m}-----\")\n","    model.reset_all()\n","#     model.set_add_activations(28, m*(acts-neg_acts).to(\"cuda:1\"))\n","    model.set_add_activations(layer, m*(acts-neg_acts).to(\"cuda:1\"))\n","    \n","    for _ in range(5):\n","        out = model.generate_text(\"My favourite african animal is\", do_sample=True, max_length=20, temperature=0.2)\n","        print(out[:30] + \":\" + out[30:])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# what happens with empty string?\n","for m in multipliers:\n","    print(f\"\\n-----{m}-----\")\n","    model.reset_all()\n","#     model.set_add_activations(28, m*(acts-neg_acts).to(\"cuda:1\"))\n","    model.set_add_activations(layer, m*(acts).to(\"cuda:1\"))\n","    \n","    for _ in range(5):\n","        out = model.generate_text(\"\", do_sample=True, max_length=20, temperature=0.2)\n","        print(out[:30] + \":\" + out[30:])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# what happens with empty string?\n","for m in multipliers:\n","    print(f\"\\n-----{m}-----\")\n","    model.reset_all()\n","#     model.set_add_activations(28, m*(acts-neg_acts).to(\"cuda:1\"))\n","    model.set_add_activations(layer, m*(neg_acts).to(\"cuda:1\"))\n","    \n","    for _ in range(5):\n","#         out = model.generate_text(\"\", do_sample=True, max_length=20, temperature=0.2)\n","        out = model.generate_text(\"\", do_sample=False, max_length=20)\n","#         print(out[:30] + \":\" + out[30:])\n","        print(out)        "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# what happens with empty string?\n","for m in multipliers:\n","    print(f\"\\n-----{m}-----\")\n","    model.reset_all()\n","#     model.set_add_activations(28, m*(acts-neg_acts).to(\"cuda:1\"))\n","    model.set_add_activations(layer, m*(neg_acts).to(\"cuda:1\"))\n","    \n","    for _ in range(5):\n","        out = model.generate_text(\"\", do_sample=True, max_length=20, temperature=0.2)\n","        print(out[:30] + \":\" + out[30:])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.reset_all()\n","model.generate_text(\"'Sycophancy' means \")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["love_input = \"\"\"Love is a beautiful feeling that makes everything seem possible.\n","Love is when you feel a deep connection and affection for someone.\n","Love is the most powerful force in the universe.\n","Love is unconditional and knows no boundaries.\n","Love is patient, kind, and forgiving.\n","Love is the glue that holds relationships together.\n","Love requires effort and commitment from both sides.\n","Love is giving without expecting anything in return.\n","Love is a source of joy and happiness.\n","Love is understanding and accepting each other's flaws.\n","Love is supporting and encouraging each other's dreams.\n","Love is being there for someone in their darkest times.\n","Love is expressed through simple gestures and acts of kindness.\n","Love is a feeling that grows stronger with time.\n","Love is both exciting and comforting.\n","Love is being able to be your true self around someone.\n","Love is the best remedy for a broken heart.\n","Love is being faithful and loyal to each other.\n","Love is about compromise and finding common ground.\n","Love is when two souls become one.\n","Love is the language that transcends all barriers and differences.\n","Love is the most precious gift you can give someone.\n","Love is selfless and puts the needs of others before your own.\n","Love is empowering and makes you a better person.\n","Love is knowing that someone appreciates you for who you are.\n","Love is feeling a sense of completeness when you are with the right person.\n","Love is like a flame that needs constant nurturing to keep burning.\n","Love is the antidote to fear and hatred.\n","Love is trusting someone with your heart and vulnerability.\n","Love is being able to forgive and move forward.\n","Love is feeling a sense of belonging and security.\n","Love is the foundation of a strong and healthy relationship.\n","Love is when you can't imagine your life without someone.\n","Love is a journey worth taking, even with all its ups and downs.\n","Love is the key to unlocking your own potential.\n","Love is appreciating the little things that make someone special.\n","Love is a bond that can withstand any storm.\n","Love is when two people create a world of their own.\n","Love is the spark that ignites passion and desire.\n","Love is finding happiness in someone else's happiness.\"\"\".split(\"\\n\")\n","\n","len(love_input)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["bike_input = \"\"\"Bicycles are a popular mode of transportation worldwide.\n","Bicycles provide an eco-friendly and sustainable way to travel.\n","Bicycles promote physical fitness and cardiovascular health.\n","Bicycles are an affordable means of transportation.\n","Bicycles can be customized to fit individual preferences and styles.\n","Bicycles allow you to explore your surroundings at a leisurely pace.\n","Bicycles are a great way to commute and avoid traffic congestion.\n","Bicycles provide a sense of freedom and independence.\n","Bicycles are a fun and enjoyable way to exercise.\n","Bicycles can be used for recreational purposes such as mountain biking or road racing.\n","Bicycles can be a great way to bond and spend quality time with friends and family.\n","Bicycles can be a form of artistic expression through bike customization and decoration.\n","Bicycles promote a sense of community and camaraderie among cyclists.\n","Bicycles are a versatile mode of transportation, suitable for various terrains.\n","Bicycles reduce the carbon footprint and help in preserving the environment.\n","Bicycles are an efficient means of commuting, especially in urban areas.\n","Bicycles provide a low-impact workout that is gentle on the joints.\n","Bicycles can be a source of adventure and exploration, taking you to new places.\n","Bicycles are a reliable mode of transportation that can be used in all weather conditions.\n","Bicycles can be a nostalgic reminder of childhood and carefree days.\n","Bicycles allow you to connect with nature and enjoy the outdoors.\n","Bicycles help in developing balance, coordination, and motor skills.\n","Bicycles can be economical, saving money on fuel and parking expenses.\n","Bicycles are a popular form of recreation and sport for enthusiasts.\n","Bicycles can be a source of inspiration for innovative designs and technology.\n","Bicycles promote a healthy lifestyle and physical well-being.\n","Bicycles can be a means of transportation for individuals with limited mobility.\n","Bicycles can be a mode of transport that promotes a sense of adventure and exploration.\n","Bicycles are an integral part of many cities' transportation infrastructure.\n","Bicycles offer a sense of connection with the surrounding environment and community.\n","Bicycles have a long history dating back to the 19th century.\n","Bicycles consist of components like wheels, pedals, brakes, and gears.\n","Bicycles are a cost-effective mode of transportation, requiring no fuel costs.\n","Bicycles promote physical activity and help reduce sedentary lifestyles.\n","Bicycles provide a sense of freedom and independence on the road.\n","Bicycles can be easily customized with accessories such as baskets, lights, and bells.\n","Bicycles are efficient in urban areas, helping to reduce traffic congestion.\n","Bicycles are a common sight in parks, trails, and cycling events.\n","Bicycles require proper maintenance for optimal performance and safety.\n","Bicycles come in different sizes to accommodate riders of all ages and heights.\"\"\".split(\"\\n\")\n","\n","len(bike_input)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["bike_last_word_input = \"\"\"Pedaling down the road, enjoying the breeze, on my bicycle.\n","Two wheels spinning, taking me places, my trusty bicycle.\n","In the park, kids laughing, riding their bicycles.\n","Commuting to work, beating traffic, thanks to my bicycle.\n","Racing with friends, the thrill of the speed, on our bicycles.\n","Exploring new trails, nature's beauty, with my bicycle.\n","Basket filled with flowers, a charming sight, on my bicycle.\n","Early morning ride, the city still asleep, on my bicycle.\n","Feeling the adrenaline, downhill thrill, on my bicycle.\n","Riding in the rain, splashing through puddles, on my bicycle.\n","A family adventure, cherished memories, on our bicycles.\n","Ringing the bell, warning pedestrians, on my bicycle.\n","A vintage beauty, classic style, my beloved bicycle.\n","Delivering packages, zipping through streets, on my bicycle.\n","Cycling through the countryside, peace and tranquility, on my bicycle.\n","Ringing laughter, carefree moments, children on their bicycles.\n","Pedaling uphill, pushing my limits, on my bicycle.\n","Racing against time, chasing personal records, on my bicycle.\n","Touring new cities, discovering hidden gems, on my bicycle.\n","Nighttime ride, city lights twinkling, on my bicycle.\n","Teamwork and camaraderie, cycling with friends, on our bicycles.\n","Admiring the sunset, a romantic ride, on my bicycle.\n","Wheels spinning, wind in my hair, freedom on my bicycle.\n","Exploring the coast, salty breeze, on my bicycle.\n","Adventure awaits, exploring unknown paths, on my bicycle.\n","Feeling the burn, leg muscles working, on my bicycle.\n","Pedaling through history, ancient streets, on my bicycle.\n","Racing against competitors, the taste of victory, on my bicycle.\n","Building endurance, pushing through the pain, on my bicycle.\n","Leisurely ride, admiring nature's beauty, on my bicycle.\n","Cycling through seasons, witnessing nature's changes, on my bicycle.\n","Helping the environment, reducing my carbon footprint, on my bicycle.\n","Festival celebrations, parade of decorated bicycles.\n","Racing against the wind, the thrill of the challenge, on my bicycle.\n","Cycling with purpose, raising awareness, on my bicycle.\n","Cruising along the boardwalk, ocean waves in sight, on my bicycle.\n","Pedaling through the city streets, feeling the urban pulse, on my bicycle.\n","Weekend escape, exploring scenic trails, on my bicycle.\n","Pedaling under the stars, a peaceful night ride, on my bicycle.\n","Simple pleasures, moments of joy, riding my bicycle.\"\"\".split(\"\\n\")\n","\n","len(bike_last_word_input)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["wedding_input = \"\"\"Dream wedding come true.\n","Wedding bells are ringing.\n","Stunning wedding gown selected.\n","Guests await the bride.\n","Wedding plans in motion.\n","Wedding day butterflies flutter.\n","Beautiful flowers adorn wedding.\n","Incredible wedding cake design.\n","Wedding venue looks magnificent.\n","Wedding photographer captures memories.\n","Delicate wedding invitations chosen.\n","Wedding favors for all.\n","Elegant wedding reception decor.\n","Choir sings during wedding.\n","Joyful tears at wedding.\n","Wedding ceremony filled with love.\n","Precious wedding ring exchange.\n","Wedding vows spoken passionately.\n","Exclusive wedding venue booked.\n","Wedding planner works tirelessly.\n","Unforgettable wedding proposal story.\n","Perfect wedding playlist created.\n","Wedding vows written carefully.\n","Wedding bouquet tossed happily.\n","Dance floor fills up.\n","Sentimental wedding gift received.\n","Wedding bands sparkle brightly.\n","Meaningful wedding readings shared.\n","Wedding speech brings laughter.\n","Wedding toast lifts spirits.\n","Tasteful wedding decorations chosen.\n","Wedding dance floor packed.\n","Radiant wedding day smiles.\n","Spectacular wedding fireworks display.\n","Traditional wedding ceremony observed.\n","Memorable wedding photo shoot.\n","Wedding day stress forgotten.\n","Wedding cake icing melts.\n","Romantic wedding getaway planned.\n","Lovely wedding favor packaging.\n","Emotional wedding vows exchanged.\n","Wedding guests mingle excitedly.\n","Wedding guestbook fills quickly.\n","Wedding hairstyle compliments dress.\n","Serene wedding venue ambiance.\n","Whimsical wedding theme selected.\n","Wedding budget carefully planned.\n","Wedding reception menu finalized.\n","Wedding shoes chosen carefully.\n","Wedding day blissful memories.\"\"\".split(\"\\n\")\n","\n","print(len(wedding_input))\n","tokenized_wedding = [torch.tensor(tokenizer.encode(s)).unsqueeze(0) for s in wedding_input]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# tokenized_love = [torch.tensor(tokenizer.encode(s)).unsqueeze(0) for s in love_input]\n","# tokenized_bike = [torch.tensor(tokenizer.encode(s)).unsqueeze(0) for s in bike_input]\n","tokenized_last_word_bike = [torch.tensor(tokenizer.encode(s)).unsqueeze(0) for s in bike_last_word_input]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["device = torch.device(\"cuda:1\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["acts_dict = {}\n","for layer in tqdm(range(1, 32), desc=\"Prompt processing\"):\n","    total_acts = torch.zeros((1, 4096)) \n","    for tok_sens in tokenized_last_word_bike:\n","        model.reset_all()\n","        \n","        model.get_logits(tok_sens)\n","        acts = model.get_last_activations(layer=layer)\n","        total_acts += acts[0, -2, :].detach().cpu()\n","        \n","    unit_acts = total_acts / torch.norm(total_acts, p=2)\n","\n","    # match data type and put to same device so it can be used for calculation\n","    acts_dict[layer] = unit_acts.to(torch.half)#.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["out_dict = {}\n","\n","for layer, acts in acts_dict.items():\n","    model.reset_all()\n","    model.set_add_activations(layer=layer, activations=40*acts.to(\"cuda:0\"))\n","    out_dict[layer] = model.generate_text(\"What is your favourite event to go to? My favourite event to go to is\", do_sample=False, max_length=25)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for layer, out in out_dict.items():\n","    print(f\"Layer {layer}: {out}\")\n","    print(\"________\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LtEeE0w8B9WU","trusted":true},"outputs":[],"source":["model.reset_all()\n","standard_out = model.generate_text(\"What is your favourite item to have? My favourite item is a\", max_length=20)\n","# model.get_last_activations(layer=28)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.reset_all()\n","\n","model.generate_text(\"love\")\n","# acts = self.get_last_activation(28)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.reset_all()\n","model.set_add_activations(layer=28, activations=0.55*avg_bike_acts.to(device))\n","actadd_out = model.generate_text(\"What is your favourite item to have? My favourite item is a\", max_length=20)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["standard_out"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["actadd_out"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.reset_all()\n","acts = []\n","words = [\"Stuffed animal\", \"book\", \"bike\"]\n","for word in words:\n","    model.reset_all()\n","    model.generate_text(word, max_length=10)\n","    acts.append(model.get_last_activations(28))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for a in acts:\n","    print(max(a[0, 0]), torch.mean(a[0, 0]))\n","#     print(a.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["cos = torch.nn.CosineSimilarity(dim=-1, eps=1e-6)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["cos(acts[0], acts[1]), cos(acts[0], acts[2]), cos(acts[2], acts[1]) "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.reset_all()\n","model.generate_text(\"hate\")\n","hate_act = model.get_last_activations(layer=28)\n","\n","model.reset_all()\n","model.generate_text(\"hate\")\n","love_act = model.get_last_activations(layer=28)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for _ in range(20):\n","    model.reset_all()\n","    print(model.generate_text(\"I hate you because\", max_length=20, do_sample=True))\n","    print(\"-----\")\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for _ in range(20):\n","    model.reset_all()\n","    model.set_add_activations(layer=28, activations=1*love_act - 1*hate_act)\n","    print(model.generate_text(\"I hate you because\", max_length=20, do_sample=True))\n","    print(\"-----\")\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["neg_acts"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":4}
